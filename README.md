# Проект анализатора продаж через LLM (OpenAPI)

## Содержание

1. [Описание](#описание)
2. [Stack](#stack)
3. [Установка и запуск](#установка-и-запуск)
4. [Тестирование](#тестирование)
5. [Замечания](#замечания)
6. [Пример Отчёта](#пример-отчёта)
7. [Автор](#автор)

## Описание

Проект предназначен для сбора суточных данных по продажам. Основной эндпоинт принимает XML с информацией о товарах и их
продажах, проверяет корректность данных и сохраняет их для последующего анализа.

Для аналитики используется Large Language Model (LLM), которая генерирует аналитические отчеты на основе собранных
данных. LLM помогает выявить ключевые тенденции и предоставить рекомендации по данным о продажах.

Формат входных данных в XML:

```
<sales_data date="2024-01-01">
    <products>
        <product>
            <id>1</id>
            <name>Product A</name>
            <quantity>100</quantity>
            <price>1500.00</price>
            <category>Electronics</category>
        </product>
        <!-- More products... -->
    </products>
</sales_data>
```

Проверяет корректность переданных данных после чего запускает обработку отложенной через celery.

## Stack

Проект построен на следующих технологиях:

- **Python** 3.11
- **FastAPI** 0.109.1
- **Celery** 5.4.0
- **Redis** 7-alpine
- **PostgreSQL** 17

## Установка и запуск

Клонировать репозиторий и перейти в него:

```
git clone https://github.com/Alex386386/analyzer_service
cd analyzer_service/
```

Cоздать и активировать виртуальное окружение:

```
python3 -m venv venv
```

Активация виртуального окружения

```
source venv/bin/activate
```

Установить зависимости из файла requirements.txt:

```
python3 -m pip install --upgrade pip
```

```
pip install -r requirements.txt
```

Создайте файл .env со следующим содержанием:

```
OPENAI_API_KEY=<ключ для доступа к API>

APP_TITLE="Сервис анализатор продаж"

NAME_OF_API_MODEL=<имя модели>

ACCESS_TOKEN=<токен для авторизации в эндпоинтах>

DATABASE_URL=postgresql+asyncpg://<POSTGRES_USER>:<POSTGRES_PASSWORD>@db:5432/postgres # Полный адрес доступа к БД для корректной работы alembic
DB_NAME=postgres
POSTGRES_USER=<имя пользователя>
POSTGRES_PASSWORD=<пароль>
DB_HOST=db
DB_PORT=5432

REDIS_PASSWORD=<цифровой пароль для доступа к редис>
REDIS_CONNECT_URL=redis://:<REDIS_PASSWORD>@redis:6379/0 # во избежание проблем указывайте адрес вместе с паролем
CACHE_LIFE_PERIOD=<срок жизни кэша в секундах>

TEST=False
```

перейти в директорию с docker-compose.yml файлом:

```
cd infra/
```

Запустить проект:

```
sudo docker compose --env-file ../.env up --build
```

Подразумевается что на сервере где запускается проект будет запущен сервис nginx,
а следовательно для корректной работы сервера надо указать переадресацию на следующие сервисы:

172.21.0.4:8000 - FastAPI бэкэнд проекта.
172.21.0.6:5555 - dashboard flower для отслеживания задач.

Для локального тестирования можно прописать порт для контейнера бэкэнда следующим образом:

```
    ports:
      - "8000:8000"
```

В проекте реализована простейшая защита эндпоинтов от нежелательных запросов в виде Bearer токена,
сам токен храниться в файле .env в переменной ACCESS_TOKEN. Используется для доступа к любому эндпоинту.

### Пример запроса на основной эндпоинт

```bash
curl -X POST "https://<ваш домен>/" \
     -H "Authorization: Bearer <ACCESS_TOKEN>" \
     -H "Content-Type: application/xml" \
     -d '<sales_data date="2024-01-01"><products>...</products></sales_data>'
```

## Тестирование

Тестирование производится через команду запущенную в корне проекта:

```
pytest
```

## Замечания

Так же реализованы простейшие ручки для взаимодействия для проверки содержимого БД.
Подробнее можно будет проверить по адресу бэкэнда с добавлением "/docs"

Весь проект реализован с использованием асинхронной архитектуры, Исключение только функционал связанный с селери т.к.
по умолчанию селери не поддерживает выполнение асинхронных операций.

Примечание: использование Celery обусловлено требованиями технического задания.
В случае продуктовой разработки рекомендуется использование асинхронных планировщиков.

Также реализовано кэширование, которое проверяет, были ли обработаны данные с переданной датой.
Если данные были обработаны ранее, возвращается сообщение:

```json
{
  "message": "Статистика за эту дату уже собрана."
}
```

## Пример Отчёта

Пример сгенерированного отчёта:

```json
{
  "date": "2024-01-01T00:00:00Z",
  "start_prompt": "Проанализируй данные о продажах за 2024-01-01:...",
  "report": "Аналитический отчет по продажам за 2024-01-01\n1. Общая выручка...",
  "id": 1,
  "create_date": "2024-11-17T04:44:58.303799Z",
  "update_date": "2024-11-17T04:44:58.303799Z"
}
```

## Автор

- [Александр Мамонов](https://github.com/Alex386386) 
